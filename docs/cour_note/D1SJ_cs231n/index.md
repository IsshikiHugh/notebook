# [å¤§ä¸€æš‘å‡] Deep Learning for Computer Vision (CS231N)

## å‰è¨€ ğŸ“

???+ summary "è¯¾ç¨‹ä»‹ç»"
    - å› ä¸ºæˆ‘åªèƒ½æ‰¾åˆ° 17 å¹´çš„è§†é¢‘ï¼Œæ‰€ä»¥æ˜¯è·Ÿç€ 17 å¹´çš„ç‰ˆæœ¬å­¦ã€‚
    
    **Course Website**: [ğŸ”—](http://cs231n.stanford.edu/)
    
    **Course Video**: [ğŸ”—](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
    
    **Course Description**
    
    - Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka â€œdeep learningâ€) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into the details of deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement and train their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. Additionally, the final assignment will give them the opportunity to train and apply multi-million parameter networks on real-world vision problems of their choice. Through multiple hands-on assignments and the final course project, students will acquire the toolset for setting up deep learning tasks and practical engineering tricks for training and fine-tuning deep neural networks.
  
    **Assignments**

    - Assignment 1: [ğŸ”—](https://cs231n.github.io/assignments2022/assignment1/)
    - Assignment 2: [ğŸ”—](https://cs231n.github.io/assignments2022/assignment2/)
    - Assignment 3: [ğŸ”—](https://cs231n.github.io/assignments2022/assignment3/)

??? note "å‚è€ƒèµ„æ–™"
    - å­¦é•¿ç¬”è®°ï¼š[ğŸ”—](https://github.com/Zhang-Each/Awesome-CS-Course-Learning-Notes/tree/master/Stanford-CS231N-NeuralNetwork%26DL)

## Lecture 1: Introduction to Convolutional Neural Networks for Visual Recognition

!!! note ""
    - Video: [ğŸ”—](https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) 
    - Slides: [ğŸ”—](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture1.pdf) 


![](1.png)
![](2.png)

- ä¸€äº›æ—©æœŸçš„å…³äº CV çš„æ€è€ƒ

![](3.png)

- æ—©æœŸå¯¹å¦‚ä½•è¡¨ç¤ºç‰©ä½“ï¼Œè¶…è¶Šã€ŒBlock Worldã€çš„è¡¨ç¤ºæ–¹æ³•

<figure markdown>
![](4.png)
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤æ‚çš„è§†è§‰ä¿¡æ¯ç®€åŒ–ä¸ºç®€å•å¯¹è±¡çš„ç»„åˆã€‚
</figure>

äººä»¬æ„è¯†åˆ°ç›´æ¥è¯†åˆ«ç‰©ä½“æ¯”è¾ƒå›°éš¾ï¼Œäºæ˜¯æƒ³åˆ°äº† **åˆ†å‰²å›¾å½¢(image segmentation)** â€”â€”å³å…ˆåšå°†åƒç´ åˆ†ç»„ï¼š

![](5.png)

- å¯å‘ï¼šè§†è§‰è¯†åˆ«çš„é‡ç‚¹å¯ä»¥ä»è¯†åˆ«å¯¹è±¡çš„ä¸€äº›å…·æœ‰è¯†åˆ«åŠ›å’Œä¸æ˜“å˜åŒ–çš„éƒ¨åˆ†å¼€å§‹

![](6.png)

~~æœ‰ç«¯è”æƒ³ FDS çš„ Voting Tree~~

- æ€»çš„è€Œè¨€ **å¯¹è±¡è¯†åˆ«** æ˜¯ CV é¢†åŸŸçš„ä¸€ä¸ªé‡è¦è¯é¢˜
- è¯¥è¯¾ç¨‹é‡ç‚¹ä¸º **å·ç§¯ç¥ç»ç½‘ç»œ(Convolutional Neural Network / CNN)**
- å…·ä½“ç€çœ¼ç‚¹ä¸º **å›¾åƒåˆ†ç±»é—®é¢˜(image classification)**
- ä¹Ÿå°†æ¶‰åŠ **å¯¹è±¡æ£€æµ‹(object detection)**ã€**å›¾åƒå­—å¹•(image captioning)** ç­‰é—®é¢˜

## Lecture 2: Image Classification Pipeline

!!! note ""
    - Video: [ğŸ”—](https://www.youtube.com/watch?v=OoUX-nOEjG0&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2) 
    - Slides: [ğŸ”—](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf) 
    - CIFAR-10 & CIFAR-100: [ğŸ”—](https://www.cs.toronto.edu/~kriz/cifar.html) 
    - **å‰ç½®**ï¼šPython Numpy Tutorial: [ğŸ”—](https://cs231n.github.io/python-numpy-tutorial/)

- **è¯­ä¹‰é¸¿æ²Ÿ(semantic gap)**

### Data-Driven Approach

1. Collect a dataset of images and labels;
2. Use Machine Learning to train a classifier;
3. Evaluate the classifier on new images;
- å¾—åˆ°çš„æ¨¡å‹æœ‰ä¸¤ä¸ªæ ¸å¿ƒ APIï¼šä¸€ä¸ªç”¨äº **è®­ç»ƒ(train)**ï¼Œä¸€ä¸ªç”¨äº **é¢„æµ‹(predict)**ã€‚
- ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸ä»‹æ„è®­ç»ƒæ—¶é—´é•¿ï¼Œä½†å¸Œæœ›é¢„æµ‹æ•ˆç‡é«˜ã€‚

> CIFAR-10 & CIFAR-100: [ğŸ”—](https://www.cs.toronto.edu/~kriz/cifar.html) 

<figure markdown>

**Distance Metric** to compare images

![](7.png)
> æˆ‘ä»¬å°†åˆ†ç±»é—®é¢˜çœ‹ä½œåœ¨â€œç©ºé—´â€ä¸­çš„æŸ“è‰²é—®é¢˜ï¼Œç‚¹è¡¨ç¤ºè®­ç»ƒæ•°æ®ï¼Œå…¶é¢œè‰²è¡¨ç¤ºå…¶è¢«æ ‡è®°çš„åˆ†ç±»ï¼›è€Œç”»æ¿ä¸­å…¶ä»–éƒ¨åˆ†çš„é¢œè‰²åˆ™è¡¨ç¤ºå½“ç‚¹è½åœ¨è¿™ä¸ªä½ç½®æ—¶å€™ä¼šè¢«åˆ†ç±»ä¸ºå“ªä¸€ç§ã€‚

![](8.png)
> ä¸­é—´â€œé»„è‰²éƒ¨åˆ†â€è¿™ç§å­¤ç«‹çš„å°å²›åœ¨å®é™…å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹å·¥ä½œæ—¶å¯èƒ½ä¸æ˜¯å¾ˆå¥½ã€‚

![](9.png)
> è¿™äº›éƒ¨åˆ†å¯èƒ½æœ‰å™ªå£°æˆ–æ˜¯è™šå‡çš„ã€‚             

</figure>

=== "**L1 (Manhattan) distance**:  _(stupid in most cases)_"
    !!! info ""
        - $d_1(I_1,I_2)=\sum_{p}|I_1^{p}-I_2^{p}|$

        <center> ![](10.png)![](11.png) </center>
        
        - å¦‚æœå›¾åƒæ—‹è½¬ï¼Œé¢„æµ‹ç»“æœä¼šå‘ç”Ÿæ”¹å˜ã€‚

=== "**L2 (Euclidean ) distance**:  _(better by comparison)_"
    !!! info ""
        - $d_2(I_1,I_2)=\sum_{p}\sqrt{(I_1^p-I_2^p)^2}$

        <center> ![](12.png)![](13.png)</center>

---

**K-Nearest Neighbors**: [ğŸ”—](https://zh.wikipedia.org/wiki/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95)

> _Interactive Demo_: [ğŸ”—](http://vision.stanford.edu/teaching/cs231n-demos/knn/)

Instead of copying label from nearest neighbor, take **majority vote** from K closest points.

![](14.png)
> ä¾‹å¦‚ï¼ŒK=1 æ—¶ä¸­é—´çš„é»„è‰²åŒºåŸŸç”±äºé™„è¿‘éƒ½æ˜¯ç»¿ç‚¹ï¼Œæ‰€ä»¥åœ¨ K å¢é•¿çš„æ—¶å€™ç»¿è‰²åœ¨è®¡ç®—ä¸­çš„æƒé‡å˜å¤§ï¼Œæ‰€ä»¥æœ€åè¢«æ ‡è®°ä¸ºç»¿è‰²ã€‚s

- _å½“ç„¶ï¼Œè¿™ç§é€šè¿‡æ¯”è¾ƒâ€œè·ç¦»â€çš„åˆ†ç±»æ–¹æ¡ˆå¹¶ä¸ä»…é™äºå›¾ç‰‡ç­‰ï¼Œå¯¹äºä»»ä½•éœ€è¦åˆ†ç±»çš„æ•°æ®ï¼Œä¾‹å¦‚æ–‡æœ¬ï¼Œåªè¦èƒ½å®šä¹‰èƒ½å¤Ÿé‡åŒ–çš„â€œè·ç¦»â€ä»¥åŠä¸€ç³»åˆ—ç›¸åº”çš„è§„åˆ™ï¼Œå°±èƒ½ä½¿ç”¨è¿™ç§æ–¹æ³•æ¥è¿›è¡Œåˆ†ç±»ã€‚_
- ç„¶è€Œï¼Œ**K-ä¸´è¿‘ç®—æ³•** åœ¨å›¾åƒåˆ†ç±»ä¸Šå‡ ä¹ä¸æ€ä¹ˆä½¿ç”¨ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒå®é™…ä½¿ç”¨èµ·æ¥ï¼Œ<u>é¢„æµ‹æ•ˆç‡è¾ƒä½</u>ï¼›ä¸”<u>â€œè·ç¦»åº¦é‡â€å¹¶ä¸éå¸¸é€‚åˆå›¾åƒå¤„ç†</u>_ï¼ˆå®ƒæ— æ³•å®Œæ•´æè¿°å›¾åƒä¸Šçš„è·ç¦»ä¿¡æ¯æˆ–å›¾åƒä¹‹é—´çš„å·®å¼‚ï¼‰_ï¼›æ­¤å¤–å®ƒè¿˜æœ‰ä¸€ä¸ªæ¯”è¾ƒä¸¥é‡çš„é—®é¢˜ï¼š**ç»´æ•°ç¾éš¾(curse of dimensionality)** [ğŸ”—](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE) _ï¼ˆå› ä¸ºåªæœ‰è®­ç»ƒæ ·æœ¬è¶³å¤Ÿå¯†é›†ï¼ŒK-ä¸´è¿‘ç®—æ³•æ‰èƒ½æ­£å¸¸è¿è¡Œï¼‰_ã€‚

---

### Hyperparameters

- Choices about the algorithm that we **set rather than learn**.

- eg: best **k** to use; best **distance** to use (L1/L2);

**Setting Hyperparameters (è°ƒå‚)**

![](15.png)
> å³ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è¶³å¤Ÿçš„è®­ç»ƒé›†ï¼Œå¹¶é€šè¿‡éªŒè¯é›†è¿›è¡Œè°ƒå‚ï¼Œå¹¶åœ¨ä¸€åˆ‡éƒ½å®Œæˆä»¥åæ‰ä½¿ç”¨æµ‹è¯•é›†æ¥éªŒè¯æ¨¡å‹çš„å‡†ç¡®åº¦ã€‚

![](16.png)

---

### Linear Classification

Parametric Approach

$$
f(x,W)=Wx+b
$$

![](17.png)
> è¿™é‡Œçš„ 10 numbers è¡¨ç¤ºçš„æ˜¯ CIFAR-10 ä¸­çš„ 10 ä¸ªç±»åˆ«å¯¹åº”çš„å¾—åˆ†ã€‚

- å³ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥åŒ…å«å›¾åƒæ•°æ® $x$ å’Œæƒé‡å‚æ•° $W$ï¼Œæ»¡è¶³å…¶è®¡ç®—ç»“æœä¸ºå„ä¸ªç±»åˆ«çš„é¢„æµ‹å¾—åˆ†
- æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ¨¡ç‰ˆï¼Œå®ƒå°†å°è¯•æ€§åœ°é€‚åº”è¯¥ç±»é‡Œå°½å¯èƒ½å¤šçš„æ ·æœ¬

![](18.png)

![](19.png)
> å¦‚æœå°†é«˜ç»´ç©ºé—´çš„æƒ…å†µæ˜ å°„åˆ°å¹³é¢çš„å‡ ä½•è§’åº¦æ¥ç†è§£ï¼Œå°±å¥½åƒåœ¨åˆ’ä¸€é“é“ç›´çº¿æ¥è¿›è¡Œåˆ’åˆ†ã€‚

- ä»è¿™ç§è§’åº¦æ¥ç†è§£å°±å¾ˆå®¹æ˜“å‘ç°ï¼Œå•ä¸€çš„çº¿æ€§åˆ†ç±»å…·æœ‰å±€é™æ€§ï¼Œä¾‹å¦‚å¯¹äºå¤šæ¨¡æ€çš„æ•°æ®ï¼Œä½¿ç”¨å•ä¸€çš„çº¿æ€§åˆ†ç±»å¯èƒ½ä¼šæ¯”è¾ƒåƒåŠ›ã€‚

## Lecture 3: Loss Functions and Optimization

!!! note ""
    - Video: [ğŸ”—](https://www.youtube.com/watch?v=h7iBpEHGVNc&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=3) 
    - Slides: [ğŸ”—](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf) 

- Linear classifier is an example of parametric classifier.
- æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£ Linear Classifation ä¸­çš„ $W$ï¼šçŸ©é˜µä¸­çš„æ¯ä¸€ä¸ªå‚æ•°è¡¨ç¤ºäº†æ¯ä¸€ä¸ªåƒç´ ç‚¹(å•ä¸ªé¢œè‰²é€šé“)å¯¹äºè¯†åˆ«æŸä¸ªç±»çš„è´¡çŒ®æƒé‡ã€‚

- A loss function that quantifies our unhappiness with the scores across the training data, tells how good our current classifier is. 
- Given a dataset of examples $\{(x_1,y_i)\}_{i=1}^N$, where $x_i$ is image and $y_i$ is label. And loss over the dataset is a sum of loss over examples: $L = \frac{1}{N}\sum L_i(f(xi,W),y_i)$.
- Loss function æ˜¯ç”¨æ¥åº¦é‡ $W$ çš„åˆé€‚ç¨‹åº¦çš„ï¼Œæˆ‘ä»¬é€šè¿‡å¯»æ‰¾åœ¨ $W$ ç©ºé—´ä¸­æŸå¤±å‡½æ•°å–æœ€å€¼æ—¶çš„ $W$ æ¥æ‰¾åˆ°æˆ‘ä»¬è®¤ä¸ºæœ€åˆé€‚çš„ $W$ã€‚

### Multi-class SVM Loss

å¦‚ä½•ç†è§£é‚£å¼ å›¾ï¼Ÿ(hinge loss?)

Let $s = f(x_i,W)$, then SVM loss is:

$$
L_i = \sum_{j \not = y_i } \left\{
    \begin{align*}
        &0              &\text{if } s_{y_i} \geq s_j + 1\\
        &s_j-s_{y_i}+1  &\text{otherwise}
    \end{align*}
\right. = \sum_{j \not = y_i } \text{max}(0,s_j-s_{y_i}+1)
$$

- ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºæŸä¸€ä¸ªæ ·æœ¬ï¼Œå®ƒå®é™…ç±»åˆ«å¯¹åº”çš„å¾—åˆ†å¦‚æœè¿œå¤§äºï¼ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¾¹ç•Œ ï¼Œå°±æ˜¯ä¸Šå›¾ä¸­çš„$+1$ï¼‰æŸä¸ªå…¶ä»–ç±»åˆ«çš„å¾—åˆ†ï¼Œé‚£ä¹ˆè¯¥â€œå…¶ä»–ç±»åˆ«â€å¯¹æŸå¤±å‡½æ•°çš„è´¡çŒ®å³ä¸º$0$ï¼›åä¹‹ï¼Œå¦‚æœå¹¶æ²¡æœ‰è¿œå¤§äºå…¶ä»–æŸä¸ªç±»åˆ«çš„å¾—åˆ†ï¼Œåˆ™éœ€è¦å°†è¿™ä¸ªåå·®ä½œä¸ºå¯¹æŸå¤±å‡½æ•°çš„è´¡çŒ®ã€‚

![](20.png)
![](21.png)

```python
def L_i_Vectorized(x, y, W):
    scores = W.dot(x)
    margins = np.maximum(0, scores - scores[y] + 1)
    margins[y] = 0
    loss_i = np.sum(margins)
    return loss_i
```

- Overview

$$
f(x,W) = Wx\\
L = \frac{1}{N}\sum_{i=1}^{n}\sum_{j\not = y_i}\text{max}(0,f(x;W)_j-f(x_i;W)_{y_i}+1)
$$

![](22.png)

- However, for that we only calculate a loss in terms of the data, some strange things like **overfitting** will happen.

![](23.png)

- è“ç‚¹ä¸ºæ¨¡å‹è®­ç»ƒæ•°æ®ï¼Œç»¿è‰²çš„ä¸ºéªŒè¯æˆ–è€…å®é™…æ•°æ®ç­‰ã€‚
- è“è‰²çš„ä¸ºè¿‡æ‹Ÿåˆåæ¨¡å‹è®­ç»ƒå‡ºæ¥çš„é¢„æµ‹è¶‹åŠ¿ï¼Œä»–ä»¬å®Œå…¨ç¬¦åˆè®­ç»ƒæ¨¡å‹çš„æ•°æ®ï¼Œä½†æ˜¯å¯ä»¥å‘ç°ï¼Œç»¿è‰²çš„çº¿æ¡æ‰æ˜¯å®é™…çš„æˆ‘ä»¬å¸Œæœ›å¾—åˆ°çš„è¶‹åŠ¿ã€‚
- è¿™ç§é¢„æµ‹ç»“æœè¿‡åº¦æ‹Ÿåˆäº†è®­ç»ƒæ•°æ®çš„è¡Œä¸ºåŠä¸ºè¿‡æ‹Ÿåˆã€‚

To solve it, we use **regularization**.

![](24.png)
> The regularization term.

![](25.png)
> Occam's Razor

- The regularization term encourages the model to somehow pick a simpler $W$ depending on the dask and the model.

So your standard loss function usually has two terms, a data loss and a regularization loss, and there's a **hyperparameter**(mentioned already in L2) here, lambda, which trades off between the two.

### Regularization

There are many regularizations used in practice, and the most common one is probably **L2 regularization** or **weight decay**. 

In common use:

- L2 regularization: $R(W) = \sum_{k}\sum_lW_{k,l}^2$;
- L1 regularization: $R(W) = \sum_k\sum_l |W_{k,l}|$;
- Elastic net (L1+L2): $R(W) = \sum_k\sum_l (\beta W_{k,l}^2+|W_{k,l}|)$;
- Max norm regularization
- Dropout
- Fancier: Batch normalization, stochastic depth...

The whole idea of regularization is just any thing that you do to your model, that sort of penalizes somehow the complexity of the model, rather than explicitly trying to fit the training data.
Each regularization has its own feature, you should choose them depends on the problems.

### Softmax Classifier (Multinomial Logistic Regression) 

- It normalizes the scores to a probability distribution.
   - Then we just want the probablity of the true class is high and as close to one.
- scores = unnormalized log probabilities of the classes
-  $P(Y=k|X=x_i)=\frac{e^{s_k}}{\sum_j e^{s_j}}\;,\;\;where\;s=f(x_i;W)$;
- That is $L_i=-\log P(Y=y_i|X=x_i)=-\log(\frac{e^{s_k}}{\sum_j e^{s_j}})$;

![](26.png)
> eg for Softmax Classifier.

![](27.png)
> Compare the two.


- åœ¨å®é™…ä½¿ç”¨ä¸­ï¼ŒSVM åœ¨ä¿è¯çœŸå®æ ‡ç­¾å¯¹åº”çš„å¾—åˆ†é«˜äºå…¶ä»–å¾—åˆ†ä¸€å®šé‡åå°±æ¥å—äº†ï¼Œå³å­˜åœ¨ä¸€ä¸ªæ˜ç¡®çš„çªå˜æ ‡å‡†ï¼›è€Œå¯¹äº Softmax æ¥è¯´ï¼Œå®ƒä¼šåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¸æ–­å°†æ­£ç¡®æ ‡ç­¾å¯¹åº”çš„æ¦‚ç‡å‘$1$é€¼è¿‘ï¼Œä¸æ–­ä¼˜åŒ–è‡ªå·±ã€‚

![](28.png)

### Optimization

- Strategy #1: A first _very bad _idea solution: Random search
- Strategy #2: **Gradient Descent** / Follow the **slope**(gradient$\nabla$)
   - Always use analytic gradient, but check implementation with numerical gradient. This is called a **gradient check.**

```python
## Vanilla Gradient Descent

while True:
    weights_grad = evaluate_gradient(loss_fun, data, weight)
    weights += - step_size * weights_grad ## perform parameter update
```
`step_size` is a **hyperparameter** here, it's also called a **learning rate** sometimes.
**Stochastic Gradient Descent (SGD)**
- The basic Gradient Descent will be messy when $N$ is large!
- So we need to start with approximate sum using a **minibatch** of examples.
   - 32 / 64 / 128 common

```python
## Vanilla Gradient Descent

while True:
    data_batch = sample_training_data(data, 256) ## sample 256 examples
    weights_grad = evaluate_gradient(loss_fun, data_batch, weight)
    weights += - step_size * weights_grad ## perform parameter update
```

- **Online Demo**: [ğŸ”—](http://vision.stanford.edu/teching/cs231n-demos/linear-classify/)

### Aside: Image Features

![](29.png)
![](30.png)
![](31.png)
![](32.png)





---

# Things below not applied yet.

## Lecture 4: Introduction to Neural Networks

[ğŸ”—](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4) Video
[ğŸ”—](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf) Slides

**Computational graphs**
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1661926558403-ceb752a5-8c67-44c0-9710-d695178096ca.png#clientId=u92d02bd5-69d0-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=244&id=ubdf6a28b&margin=%5Bobject%20Object%5D&name=image.png&originHeight=305&originWidth=848&originalType=binary&ratio=1&rotation=0&showTitle=true&size=65988&status=done&style=shadow&taskId=u93e3bb10-4af7-4863-86e2-70036ee8967&title=eg.%20for%20the%20linear%20classifier.&width=678.4 "eg. for the linear classifier.")
**Backpropagation**
é€šè¿‡å°†ç®—å¼å†™æˆè¿™ç§â€œèŠ‚ç‚¹å›¾â€çš„å½¢å¼ï¼Œå¯ä»¥è¿›ä¸€æ­¥è®©æˆ‘ä»¬çœ‹æ¸…è®¡ç®—è¿‡ç¨‹å¹¶æ–¹ä¾¿æˆ‘ä»¬è®¡ç®—æ¢¯åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œä¾‹å¦‚$f(x,y,z)=(x+y)z$ï¼Œå°†å…¶å†™æˆèŠ‚ç‚¹å›¾å¦‚ä¸‹ï¼Œå¹¶è®¡ç®—æ‰€éœ€è¦çš„å‚æ•°ã€‚
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662094352514-ccc05dd7-9f2d-4373-96ad-cb538ed4ac71.png#clientId=u940a9301-d778-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=170&id=u6aff39f2&margin=%5Bobject%20Object%5D&name=image.png&originHeight=218&originWidth=476&originalType=binary&ratio=1&rotation=0&showTitle=false&size=33927&status=done&style=shadow&taskId=ud2fff276-4279-4736-a038-aad6b784d88&title=&width=371)     ![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662094404407-789cc324-198b-438e-8682-ea1149902883.png#clientId=u940a9301-d778-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=170&id=u75333fad&margin=%5Bobject%20Object%5D&name=image.png&originHeight=253&originWidth=451&originalType=binary&ratio=1&rotation=0&showTitle=false&size=50212&status=done&style=shadow&taskId=u722d9937-32b7-4c64-8d39-d5ebad0ed80&title=&width=303)
ç„¶åæˆ‘ä»¬ä»å›¾çš„æœ«ç«¯å¼€å§‹è®¡ç®—ï¼Œå¾—åˆ°è¿™äº›èŠ‚ç‚¹æ•°æ®ï¼š

$\left\{
\begin{aligned}
&\frac{\partial{f}}{\partial{f}}=1,\\
&\frac{\partial{f}}{\partial{z}}=q=3,\\
&\frac{\partial{f}}{\partial{q}}=z=-4,\\
&\frac{\partial{f}}{\partial{x}}=\frac{\partial{f}}{\partial{q}}\frac{\partial{q}}{\partial{x}}=-4,\\
&\frac{\partial{f}}{\partial{y}}=\frac{\partial{f}}{\partial{q}}\frac{\partial{q}}{\partial{y}}=-4
\end{aligned}
\right.$

![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662094787671-9428b3d4-e6ca-4f7e-8453-a6b827a7abc7.png#clientId=u940a9301-d778-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=106&id=CyxhW&margin=%5Bobject%20Object%5D&name=image.png&originHeight=320&originWidth=492&originalType=binary&ratio=1&rotation=0&showTitle=false&size=41320&status=done&style=shadow&taskId=u38e8ac2b-4ae1-4ade-9fb6-74a73e4a217&title=&width=163)![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662094798526-b165d7ba-7361-415d-8ba2-0813f367ec77.png#clientId=u940a9301-d778-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=106&id=u8aa60156&margin=%5Bobject%20Object%5D&name=image.png&originHeight=338&originWidth=487&originalType=binary&ratio=1&rotation=0&showTitle=false&size=43056&status=done&style=shadow&taskId=u4dd0839b-d125-45d7-a171-8ed67b01999&title=&width=153)![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662094841227-3bf6ad6b-d658-4858-9474-f9c9ca7432eb.png#clientId=u940a9301-d778-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=106&id=u34c509f2&margin=%5Bobject%20Object%5D&name=image.png&originHeight=342&originWidth=489&originalType=binary&ratio=1&rotation=0&showTitle=false&size=46360&status=done&style=shadow&taskId=u18bfcf23-f6ef-49a9-8236-b4296057635&title=&width=152)
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662094949463-9efb77c5-9360-4c4e-8bfe-3e322bf774b9.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=106&id=u07a281fd&margin=%5Bobject%20Object%5D&name=image.png&originHeight=321&originWidth=490&originalType=binary&ratio=1&rotation=0&showTitle=false&size=47118&status=done&style=shadow&taskId=u7dc6d18c-9081-48f9-8075-29baefefd8e&title=&width=162)![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662094959583-24c94484-9504-4813-b357-feaa08315ba8.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=106&id=u8dda4ea8&margin=%5Bobject%20Object%5D&name=image.png&originHeight=314&originWidth=490&originalType=binary&ratio=1&rotation=0&showTitle=false&size=48929&status=done&style=shadow&taskId=u49fe4b6d-87e6-43c6-b0f3-5ea040056f9&title=&width=165)

![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662095196409-e727181a-98ef-4e32-9ae5-328ee7b6f748.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=377&id=uf5deedcc&margin=%5Bobject%20Object%5D&name=image.png&originHeight=471&originWidth=960&originalType=binary&ratio=1&rotation=0&showTitle=true&size=129451&status=done&style=shadow&taskId=u7a87ccce-347e-41f9-b746-8c83c3768f1&title=backprop%20%28red%20lines%29&width=768 "backprop (red lines)")
å¯ä»¥å‘ç°ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—è¿‡ç¨‹ä¸­åªéœ€è¦å°†â€œç›¸é‚»â€æ¢¯åº¦ä¹˜ä»¥"local gradient"å³å¯è®¡ç®—å‡ºæ‰€éœ€è¦çš„æ–°çš„"local gradient"ã€‚è€Œåªéœ€è¦å†æ²¿ç€è·¯å¾„å†å°†æ‰€æœ‰çš„"local gradient"ç´¯ä¹˜èµ·æ¥ï¼Œå°±èƒ½å¾—åˆ°æ¯ä¸€ä¸ªå˜é‡å…³äºè¡¨è¾¾å¼çš„æ¢¯åº¦ã€‚
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662095551007-8758a9b5-6a11-4169-982e-98ddb1ef5b22.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u73a3cc75&margin=%5Bobject%20Object%5D&name=image.png&originHeight=554&originWidth=1158&originalType=binary&ratio=1&rotation=0&showTitle=false&size=207665&status=done&style=shadow&taskId=uf611daf5-5775-43ac-b0a5-9970fe2376c&title=)
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662095644605-355df07d-77a8-4dc5-ba17-615a6d18f70a.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=195&id=u3b8ae8a9&margin=%5Bobject%20Object%5D&name=image.png&originHeight=350&originWidth=588&originalType=binary&ratio=1&rotation=0&showTitle=false&size=68127&status=done&style=shadow&taskId=ubf50001a-a11c-43bd-a2be-03efd55e5ce&title=&width=328)![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662095787716-82499f8d-e8dd-4b40-b744-f276b532a0a1.png#clientId=u75e002cd-f71b-4&crop=0.1742&crop=0&crop=0.9703&crop=1&from=paste&height=155&id=uffb2b043&margin=%5Bobject%20Object%5D&name=image.png&originHeight=256&originWidth=646&originalType=binary&ratio=1&rotation=0&showTitle=false&size=50817&status=done&style=shadow&taskId=u5b56d5bc-cca0-4134-99df-03acf05582f&title=&width=392)
ç‰¹åˆ«çš„ï¼Œç”±äºèŠ‚ç‚¹æ˜¯æˆ‘ä»¬è®¤ä¸ºå®šä¹‰çš„ï¼Œè€Œä¸”è¯¥æ–¹æ³•æ‰€ä¾èµ–â€œé“¾å¼æ³•åˆ™â€ä¹Ÿå…è®¸å‡½æ•°çš„è‡ªç”±ç»„åˆï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥äººä¸ºâ€œåˆå¹¶/åˆ†å‰²â€ä¸€äº›èŠ‚ç‚¹ï¼Œä¾‹å¦‚åœ¨ä¸Šé¢é‚£ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€åå››ä¸ªèŠ‚ç‚¹åˆå¹¶ä¸ºä¸€ä¸ª"sigmoid function"(i.e. $\frac{1}{1+e^{-x}}$)ã€‚
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662096256516-8a201b6d-a028-4b36-8bfa-2007713196ec.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=422&id=u01e9dfa8&margin=%5Bobject%20Object%5D&name=image.png&originHeight=528&originWidth=1080&originalType=binary&ratio=1&rotation=0&showTitle=false&size=189451&status=done&style=shadow&taskId=u2869bd4c-67f2-43ca-81cf-2f8d88c61fd&title=&width=864)
**Patterns in backward flow**
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662096486759-666c3153-1716-421c-93b4-0e040cb94ebd.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=310&id=u273883e7&margin=%5Bobject%20Object%5D&name=image.png&originHeight=388&originWidth=1111&originalType=binary&ratio=1&rotation=0&showTitle=true&size=146199&status=done&style=shadow&taskId=u8a07f74b-2776-404b-a756-93c53edbd10&title=%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A7%8D%E7%90%86%E8%A7%A3%E7%AE%97%E7%AC%A6%E5%9C%A8%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8%E7%9A%84%E6%80%9D%E8%B7%AF%E3%80%82&width=888.8 "å¦å¤–ä¸€ç§ç†è§£ç®—ç¬¦åœ¨è®¡ç®—å›¾ä¸­çš„ä½œç”¨çš„æ€è·¯ã€‚")
å¯ä»¥å‘ç°ï¼Œ`max()`è¿ç®—åœ¨æ¢¯åº¦ä¼ é€’è¿‡ç¨‹ä¸­åªèµ·åˆ°è·¯ç”±å™¨çš„ä½œç”¨ï¼Œä¹Ÿå°±æ˜¯è¯´å°†å…¶ä¼ é€’åˆ°è¾ƒå¤§çš„é‚£ä¸ªå˜é‡é‚£ä¸€ä¾§ï¼Œä½†ä¸æ”¹å˜æ¢¯åº¦çš„å€¼ï¼›è€Œå¯¹äºè¾ƒå°çš„é‚£ä¸ªå˜é‡ï¼Œæ¢¯åº¦ä¼ é€’è¢«é˜»æ–­ï¼Œç›®æ ‡å˜é‡çš„æ¢¯åº¦ä¸º$0$ã€‚
å°†è¿™ä¸ªæµç¨‹è¿ç§»åˆ°ç¥ç»ç½‘ç»œä¸Šï¼Œæˆ‘ä»¬åªéœ€è¦å°†è¿™äº›æ•°å­—å˜ä¸º Jacobian matrix å³å¯ã€‚
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662097315555-bc421009-62af-46e7-a83a-23ac210a7543.png#clientId=u75e002cd-f71b-4&crop=0&crop=0.0084&crop=1&crop=1&from=paste&height=176&id=ue7867603&margin=%5Bobject%20Object%5D&name=image.png&originHeight=562&originWidth=1144&originalType=binary&ratio=1&rotation=0&showTitle=false&size=133802&status=done&style=shadow&taskId=u77b5c463-8f22-4cbc-8cd2-e40e831e9ab&title=&width=359) ![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662097300960-2880c190-f74f-4f4c-a28d-5b1b764c9e91.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=175&id=aMjPy&margin=%5Bobject%20Object%5D&name=image.png&originHeight=563&originWidth=1148&originalType=binary&ratio=1&rotation=0&showTitle=false&size=181001&status=done&style=shadow&taskId=u29a62e1b-f8b0-48ae-b7ab-36df39f1e64&title=&width=357)
- Always check: The **gradient** with respect to a variable should have the **same shape** as the variable.
   - Because each element of your gradient is **quantifying** **how much** that element is **contributing** to your final output.

åœ¨å…·ä½“å®ç°ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šå®ç°ä¸€ä¸ª`forward()`ç”¨äºè®¡ç®—å‡½æ•°çš„è¾“å‡ºï¼Œä»¥åŠä¸€ä¸ª`backward()`ç”¨äºæŒ‰ç…§ä¸Šé¢æåˆ°çš„æ–¹æ³•è®¡ç®—æ¢¯åº¦ã€‚
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662097899735-c76fadf6-326c-4cce-85e3-96438fbcaa31.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=192&id=u4e60e55e&margin=%5Bobject%20Object%5D&name=image.png&originHeight=447&originWidth=744&originalType=binary&ratio=1&rotation=0&showTitle=false&size=212801&status=done&style=shadow&taskId=u7a6adb3a-52f9-41c5-9045-5076578688c&title=&width=320) ![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662098058403-d9a228dc-d58e-40d0-af00-7741644aade6.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=192&id=u59b69b84&margin=%5Bobject%20Object%5D&name=image.png&originHeight=443&originWidth=913&originalType=binary&ratio=1&rotation=0&showTitle=false&size=127998&status=done&style=shadow&taskId=uae14eb05-1b7d-457d-a3e8-215fd8e881b&title=&width=396)
**Summary**
- Neural nets will be very large: impractical to write down gradient formula by hand for all parameters.
- **Backpropagation **= recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates.
- Implementations maintain a graph structure, where the nodes implement the `**forward()**` / `**backward()**` API.
   - **Forward**: compute **result **of an operation and save any intermediates needed for gradient computation in memory.
   - **Backward**: apply the chain rule to compute the **gradient **of the loss function with respect to the inputs.

**Neural Networks**
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662100500579-bcc57e07-9f5a-4c5d-b5fb-8b4458667ab5.png#clientId=u75e002cd-f71b-4&crop=0&crop=0.1307&crop=1&crop=1&from=paste&height=451&id=u3eed4861&margin=%5Bobject%20Object%5D&name=image.png&originHeight=564&originWidth=1112&originalType=binary&ratio=1&rotation=0&showTitle=false&size=256672&status=done&style=shadow&taskId=u61952589-7539-40af-8b01-c099cb4bbca&title=&width=890)
é€šè¿‡å‡½æ•°å åŠ çš„æ–¹å¼æ¥å®ç°ç¥ç»ç½‘ç»œã€‚
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662100617343-1e86dcd4-6760-4857-9a4c-46b95f46cc2a.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=451&id=u76969c7d&margin=%5Bobject%20Object%5D&name=image.png&originHeight=564&originWidth=1170&originalType=binary&ratio=1&rotation=0&showTitle=true&size=166137&status=done&style=shadow&taskId=u290f5796-e610-4a15-9beb-1cc072e5b60&title=%E4%B8%8E%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E7%9A%84%E4%B8%80%E4%B8%AA%E7%B1%BB%E6%AF%94&width=936 "ä¸ç”Ÿç‰©ç¥ç»çš„ä¸€ä¸ªç±»æ¯”")
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662100753740-27277a19-481e-4ac4-b220-f8cb6368c56a.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=0.9836&from=paste&height=455&id=u5712f1ea&margin=%5Bobject%20Object%5D&name=image.png&originHeight=568&originWidth=1126&originalType=binary&ratio=1&rotation=0&showTitle=false&size=117566&status=done&style=shadow&taskId=ud2267ddd-6185-4e94-91ca-ba59cd68bb2&title=&width=901)
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662100835551-f8e2e51e-aaae-4939-a75f-e121b0587bd9.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=178&id=u9ced8d30&margin=%5Bobject%20Object%5D&name=image.png&originHeight=549&originWidth=1159&originalType=binary&ratio=1&rotation=0&showTitle=false&size=369978&status=done&style=shadow&taskId=u5019ebe9-637a-4eab-aabf-98921bbe2e1&title=&width=376)![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662100896268-d929ed17-eb96-4cef-bb2a-cbc864956602.png#clientId=u75e002cd-f71b-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=178&id=u35f8bc77&margin=%5Bobject%20Object%5D&name=image.png&originHeight=554&originWidth=1071&originalType=binary&ratio=1&rotation=0&showTitle=false&size=428533&status=done&style=shadow&taskId=uc43ab531-583c-4c9b-ba6f-e45d1e2a081&title=&width=344)
**Summary**
- We arrange neurons into **fully-connected layers**.
- The **abstraction **of a **layer **has the nice property that it allows us to use efficient vectorized code (e.g. matrix multiplies).
- Neural networks are not really neural.

## Lecture 5: Convolutional Neural Networks
[ğŸ”—](https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=5) Video
[ğŸ”—](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf) Slides

**Convolutional Neural Networks**
ä¹‹å‰æåˆ°è¿‡çš„ Fully Connected Layer ä¼šå°†å¤šç»´çš„æ•°æ®æ‹‰ä¼¸ä¸ºå‘é‡çš„å½¢å¼ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œ Convolution Layer åˆ™ä¼šä¿ç•™è¾“å…¥æ•°æ®çš„å½¢çŠ¶**ç‰¹å¾**ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œfilter ä¼šä¸æ•°æ®å…·æœ‰ç›¸åŒçš„æ·±åº¦ï¼ˆä¾‹å¦‚ input æ˜¯ 32x32x3ï¼Œé‚£ä¹ˆ filter å¯ä»¥ä¸º 5x5x3ï¼‰ã€‚é€šè¿‡å°† filter ä¸ input çš„æŸä¸€ä¸ªåŒå½¢çŠ¶çš„å­åŒºåŸŸåšç‚¹ç§¯ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ•°å­—ã€‚
- _Convolve the filter with the image. (i.e. "slide over the image spatially, computing dot products")_

![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662778616456-4c661045-fca7-4f98-adbd-9460dd230792.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=312&id=u00fedce3&margin=%5Bobject%20Object%5D&name=image.png&originHeight=390&originWidth=843&originalType=binary&ratio=1&rotation=0&showTitle=true&size=66769&status=done&style=shadow&taskId=u08a3e835-a8e4-4e06-8b29-7205c257573&title=%E5%9C%A8%E6%9F%90%E4%B8%AA%E4%BD%8D%E7%BD%AE%E8%8E%B7%E5%BE%97%E7%82%B9%E7%A7%AF%EF%BC%8C%E5%8D%B3%20filter%20%E5%A6%82%E4%BD%95%E4%BD%9C%E7%94%A8%E4%BA%8E%20input&width=674.4 "åœ¨æŸä¸ªä½ç½®è·å¾—ç‚¹ç§¯ï¼Œå³ filter å¦‚ä½•ä½œç”¨äº input")
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662778922924-61f20e8a-64f2-4d72-8ee0-7a6fb3ab8e5e.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=175&id=u3469dbff&margin=%5Bobject%20Object%5D&name=image.png&originHeight=386&originWidth=822&originalType=binary&ratio=1&rotation=0&showTitle=false&size=44328&status=done&style=shadow&taskId=ub3b1a169-046c-4034-aeb7-c96fc8c2558&title=&width=373)  ![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662779019849-c2497280-1a95-4f95-b636-bf33d6243738.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=175&id=ucfebd984&margin=%5Bobject%20Object%5D&name=image.png&originHeight=469&originWidth=895&originalType=binary&ratio=1&rotation=0&showTitle=false&size=53574&status=done&style=shadow&taskId=u781d4018-a35a-4fa3-97d6-42662048dee&title=&width=334)
é‡‡ç”¨å¤šä¸ª filters å¹¶å°†ç»“æœå åŠ ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°å¤šä¸ªæ¿€æ´»å›¾(activation maps)ï¼Œä½œä¸ºä¸€ä¸ªå¤„ç†åçš„æ•°æ®ã€‚
- _ConvNet is a sequence of Convolution Layers, interspersed with activation functions._

![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662780472229-1aa4373d-1508-4da9-b32e-841435d8ac16.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=286&id=uff9f1496&margin=%5Bobject%20Object%5D&name=image.png&originHeight=358&originWidth=883&originalType=binary&ratio=1&rotation=0&showTitle=false&size=44369&status=done&style=shadow&taskId=ua9a50793-94ba-4856-acae-ac2c0ae8a23&title=&width=706.4)
å…³äºè¾“å‡ºçš„æ¿€æ´»å›¾çš„å¤§å°ï¼Œæœ‰å¦‚ä¸Šå…¬å¼ã€‚
é€šè¿‡é€‰æ‹©æ­¥é•¿ï¼Œæˆ‘ä»¬å¯ä»¥æ§åˆ¶æ»‘åŠ¨çš„é€Ÿç‡ã€‚ä»æŸç§æ„ä¹‰ä¸Šæ¥è®²è¿™ä¹Ÿæ˜¯åœ¨æ§åˆ¶æ»‘åŠ¨ç»“æœçš„åˆ†è¾¨ç‡ï¼Œä¹Ÿæ˜¯æ± åŒ–æ“ä½œèƒŒåçš„æ€æƒ³ä¹‹ä¸€ã€‚è€Œå…·ä½“é€‰æ‹©å¤§æ­¥é•¿è¿˜æ˜¯å°æ­¥é•¿æ˜¯ä½ éœ€è¦å¯¹å„ç§å› ç´ è¿›è¡Œè€ƒé‡å†³å®šçš„ã€‚
å¯ä»¥å‘ç°ï¼Œæ­¥é•¿ä¸º 3 æ—¶æ— æ³•åŒ¹é…ã€‚è€Œåœ¨å®è·µè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æ‰‹æ®µæ¥è§£å†³è¿™ç§æ— æ³•åŒ¹é…çš„æƒ…å†µï¼Œå¦‚"zero pad to borders"ä»¥ä½¿å¤§å°ç¬¦åˆæ­¥é•¿ã€‚
- _The zero padding __does add some sort of extraneous features at the corners__, and we're doing our best to get some value and do, like process that region of the image. And so zero padding is kind of one way to do this. There's __also other ways__ to do this that, you know, you can try and like, mirror the values here or extend them, and so it doesn't have to be zero padding, but __in practice this is one thing that works reasonably__._

![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662780648310-3a86f775-2cdd-4f20-ae3b-7ffa8a0cd8e9.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=362&id=u565fb487&margin=%5Bobject%20Object%5D&name=image.png&originHeight=452&originWidth=929&originalType=binary&ratio=1&rotation=0&showTitle=true&size=109534&status=done&style=shadow&taskId=u6279e545-571e-4ca9-95e7-ecc373d7b8d&title=%E6%B3%A8%E6%84%8F%EF%BC%8C%E9%87%87%E7%94%A8%E9%9B%B6%E6%89%A9%E5%B1%95%E4%BB%A5%E5%90%8E%E4%B8%8A%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%9A%84%E5%85%AC%E5%BC%8F%E5%B0%B1%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E4%BA%86%E3%80%82%E8%BF%99%E4%B9%9F%E6%AD%A3%E6%98%AF%E9%9B%B6%E6%89%A9%E5%B1%95%E7%9A%84%E4%B8%80%E4%B8%AA%E7%89%B9%E7%82%B9%EF%BC%8C%E5%AE%83%E5%8F%AF%E4%BB%A5%E8%AE%A9%E5%9B%BE%E5%83%8F%E7%9A%84%E5%BD%A2%E7%8A%B6%E4%B8%8D%E5%BF%85%E7%BC%A9%E5%B0%8F%E3%80%82&width=743.2 "æ³¨æ„ï¼Œé‡‡ç”¨é›¶æ‰©å±•ä»¥åä¸Šä¸€å¼ å›¾çš„å…¬å¼å°±æ— æ³•ä½¿ç”¨äº†ã€‚è¿™ä¹Ÿæ­£æ˜¯é›¶æ‰©å±•çš„ä¸€ä¸ªç‰¹ç‚¹ï¼Œå®ƒå¯ä»¥è®©å›¾åƒçš„å½¢çŠ¶ä¸å¿…ç¼©å°ã€‚")
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662781477669-28b50a57-109f-419b-a8fe-bd792abca8e3.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=190&id=u3ca29e98&margin=%5Bobject%20Object%5D&name=image.png&originHeight=237&originWidth=945&originalType=binary&ratio=1&rotation=0&showTitle=true&size=36284&status=done&style=shadow&taskId=u64ccf793-366b-4871-8eb3-e997ba4f4ac&title=%E4%B8%80%E9%81%93%E6%B5%8B%E8%AF%95%E9%A2%98%EF%BC%8C%E8%AE%A1%E7%AE%97%E8%BF%99%E6%A0%B7%E4%B8%80%E5%B1%82%E4%B8%AD%E6%9C%89%E5%A4%9A%E5%B0%91%E5%8F%82%E6%95%B0%E3%80%82&width=756 "ä¸€é“æµ‹è¯•é¢˜ï¼Œè®¡ç®—è¿™æ ·ä¸€å±‚ä¸­æœ‰å¤šå°‘å‚æ•°ã€‚")

Ans- Each filter has $5\times5\times3_{\text{(depth)}} + 1_{\text{(for bias)}} = 76$ params.
- So the total number is $76\times 10 = 760$.

**Pooling layer**
- makes the representations smaller and more manageable
- operates over each activation map independently:

![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662783361516-3f949cb8-6ea1-41dd-8450-9bfff55f04cc.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=277&id=ua85e54b7&margin=%5Bobject%20Object%5D&name=image.png&originHeight=346&originWidth=437&originalType=binary&ratio=1&rotation=0&showTitle=true&size=69724&status=done&style=shadow&taskId=u7c47bbcd-e8f4-4551-a6a6-f29d66b8a7f&title=just%20downsample&width=349.6 "just downsample")
A common way to do this is max pooling:
![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662783517400-2e3bd335-ad6d-437b-8823-743b06982cd7.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=369&id=u53f5dd65&margin=%5Bobject%20Object%5D&name=image.png&originHeight=461&originWidth=809&originalType=binary&ratio=1&rotation=0&showTitle=true&size=42815&status=done&style=shadow&taskId=u60ec636c-49b6-4554-97f7-262765e93d3&title=just%20take%20the%20max%20value&width=647.2 "just take the max value")
å¯¹äºæ± åŒ–å±‚ï¼Œåœ¨è¿›è¡Œæ»‘åŠ¨çª—å£æ—¶æˆ‘ä»¬æ›´å¸Œæœ›æ­¥é•¿çš„è®¾ç½®èƒ½ä½¿ filter æ²¡æœ‰é‡å ï¼Œä»¥æ»¡è¶³å°èŠ‚å¼€å¤´æåˆ°çš„"independently"ã€‚

**Fully Connected Layer (FC layer)**
- Contains neurons that connect to the entire input volume, as in ordinary Neural Networks.

![](https://cdn.nlark.com/yuque/0/2022/png/22387144/1662784708958-0b1cbaf5-07fb-47ab-b132-dcf2a797cdee.png#clientId=u992d708c-53fe-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=249&id=u09f3adff&margin=%5Bobject%20Object%5D&name=image.png&originHeight=311&originWidth=649&originalType=binary&ratio=1&rotation=0&showTitle=false&size=184807&status=done&style=shadow&taskId=u3d835372-5e6d-417c-b2e9-0d24d45b9bb&title=&width=519.2)

## Assignments

 - è¯·æŸ¥çœ‹å­æ–‡ç« ï¼š[Assignments](assignments/index.md)




